# ğŸ” Forgery Detection and Image Captioning using ELA, ViT, and BLIP-2

This repository contains a comprehensive solution for **detecting forged regions in images** and generating **context-aware captions** using:
- **Error Level Analysis (ELA)** for preprocessing,
- **Vision Transformer (ViT)** for feature extraction,
- **BLIP-2** for multimodal captioning.

---

## ğŸ“Œ Features

- ğŸ–¼ï¸ **Forgery Detection** using ELA heatmaps and ViT-based classification/localization
- ğŸ§  **Caption Generation** with BLIP-2 to describe manipulated content
- ğŸ“ˆ Supports CASIA 2.0 (training) and CASIA 1.0 (testing)
- ğŸš€ Compatible with GPU for efficient training/inference
- ğŸ“¦ Modular, clean PyTorch code

---

## ğŸ—‚ï¸ Project Structure

```bash
Forgery-Detection-and-Captioning/
â”œâ”€â”€ data/                     # Dataset loading and preprocessing
â”œâ”€â”€ models/                   # ViT + BLIP-2 model definitions
â”œâ”€â”€ utils/                    # Helper functions (metrics, ELA, visualization)
â”œâ”€â”€ output_dir/               # Training outputs (checkpoints, logs)
â”œâ”€â”€ pretrained-weights/       # Pretrained weights (ViT, BLIP-2)
â”œâ”€â”€ notebooks/                # Jupyter notebooks for demo/training
â”œâ”€â”€ train.py                  # Main training script
â”œâ”€â”€ evaluate.py               # Evaluation script
â”œâ”€â”€ config.yaml               # Configuration file
â””â”€â”€ README.md                 # You're here
```
## ğŸ§ª Dataset
### ğŸ“¦ CASIA 2.0 & CASIA 1.0
All images undergo ELA transformation.

Model trained on tampered vs authentic images with region mask supervision.

## ğŸ”§ Setup Instructions
### 1. Clone the repository
bash
```
git clone https://github.com/ChiragSingh01/Forgery-Detection-and-Captioning.git
cd Forgery-Detection-and-Captioning
```
### 2. Create a virtual environment
bash
```
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
```
### 3. Install dependencies
bash
```
pip install -r requirements.txt
```
## ğŸ§  Model Architecture
ViT Backbone: For ELA-based visual feature learning

BLIP-2: Used for visual-linguistic alignment and captioning

Fusion Module: Combines visual embeddings and ELA signals

Losses: BCE Loss (region prediction), Captioning Loss (language generation)

## ğŸ’» Usage
Train
```
bash
python train.py --config config.yaml
```
Evaluate
```
bash
python evaluate.py --checkpoint output_dir/best_model.pth
```
Visualize ELA
```
bash
python utils/ela_visualize.py --image path/to/image.jpg
```
## ğŸ§¾ Sample Output


## ğŸ“Š Metrics
Detection: F1 Score, IoU, Pixel Accuracy

Captioning: BLEU, METEOR, CIDEr

## ğŸ“¦ Pretrained Weights
Place pretrained ViT/BLIP-2 weights in:
```
bash
pretrained-weights/
â”œâ”€â”€ vit.pth
â”œâ”€â”€ blip2.pth
```
You may use MAE or HuggingFace BLIP-2 weights depending on availability.

## ğŸ™‹â€â™‚ï¸ Author
Chirag Singh
B.Tech IT | Delhi Technological University
ğŸ“§ chiragsingh@example.com
ğŸŒ LinkedIn | GitHub

## ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.
